{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a40a09b9-987d-4589-ab6f-6f64b6ae9e7b",
   "metadata": {},
   "source": [
    "## Bag of Words (BoQ)\n",
    "- 단어의 순서는 고려하지 않고 단어의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 방법\n",
    "    - 단어의 전후 관계는 의미가 없고 각 단어의 출현 빈도수, 즉 카운트에만 집중한다 \n",
    "- 우선, 각 단어에 고유한 정수 인덱스를 부여\n",
    "- 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만듬\n",
    "- scikit-learn 의 CountVectorizer 를 이용하여 간단히 BoW를 구성할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbae10f0-ea84-46d8-ba16-df8d12b0319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edd853a0-853c-40b1-8b56-09bd77a79faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = ['나는 배가 고프다', '내일 점심 뭐먹지','내일 공부 해야겠다','점심 먹고 공부 해야지']\n",
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aa134dd-6d21-4dc1-ad34-3bd71d8b079d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나는': 2, '배가': 6, '고프다': 0, '내일': 3, '점심': 7, '뭐먹지': 5, '공부': 1, '해야겠다': 8, '먹고': 4, '해야지': 9}\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer.fit(text_data)\n",
    "print(count_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58252f31-cd8f-415a-983c-77d1428636b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0 0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "sentence = [text_data[0]]\n",
    "print(count_vectorizer.transform(sentence).toarray())\n",
    "# 인덱스별 단어의 카운트 수 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91fe200-d6b9-4100-b393-3a830367bef0",
   "metadata": {},
   "source": [
    "### 문서 단어 행렬(Document-Term Matrix, DTM)\n",
    "- 서로 다른 문서들의 BoW들을 결합한 표현 방법  \n",
    "- 다수의 문서에 등장하는 각 단어들의 빈도를 행렬로 표현한 것  \n",
    "- 문서1 : 호기심 많은 고양이  \n",
    "- 문서2 : 꼬리가 긴 고양이  \n",
    "- 문서3 : 호기심 많은 강아지  \n",
    "- 문서4 : 철수는 동물을 좋아해요 \n",
    "|      |강아지|고양이|긴|꼬리가|동물을|많은|좋아해요|철수는|호기심|\n",
    "|------|-----|-----|--|-----|-----|---|-------|-----|-----|\n",
    "|문서1  |  0  |  1  | 0|  0  |  0 | 1  |   0   |  0  |  1  |\n",
    "|문서2  |  0  |  1  |1|  1  |  0 | 0  |   0   |  0  |  0  |\n",
    "|문서3  |  1  |  0  |0|  0  |  0 | 1  |   0   |  0  |  1  |\n",
    "|문서4  |  0  |  0  |0|  0  |  1 | 0  |   1   |  1  |  0  |  \n",
    "- 각 문서에서 등장한 단어의 빈도를 행렬값으로 표시  \n",
    "- 문서들을 서로 비교할 수 있도록 수치화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a1e5c4-b706-4c18-bb22-0e8ee75fc3b3",
   "metadata": {},
   "source": [
    "## 문서 단어 행렬이 한계\n",
    "1. 희소 표현\n",
    "- one-hot encoding 방식의 벡터는 단어 집합의 크기가 벡터의 차원이 됨(대부분의 값이 0)\n",
    "- 공간과 계산 리소스의 낭비\n",
    "\n",
    "2. 단순 빈도수 기반 접근\n",
    "- 여러 문서에 등장하는 모든 단어에 대해서 빈도수만을 사용하는 한계\n",
    "- 각 문서에는 중요한 단어와 불필요한 단어가 혼재되어 있음\n",
    "- DTM에 불용어와 중요한 단어에 대한 가중치를 부여하는 방법이 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd2cdfd-2c56-4b82-84ed-2d203b8c1fe2",
   "metadata": {},
   "source": [
    "### TF-IDF(Term Frequency-Inverse Documents Frequency)\n",
    "- 참고 페이지:\n",
    "    - https://wikidocs.net/31698\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af17bead-9bce-42af-8081-e0954737c832",
   "metadata": {},
   "source": [
    "- 단어의 빈도와 역 문서 빈도를 사용하여 DTM내의 각 단어들마다 중요한 정도의 가중치를 부여하는 방법  \n",
    "- 주로 문서의 유사도를 구하는 작업, 검색 시스템에서 검색결과의 중요도를 정하는 작업, 문서 내에서 특정 단어의 중요도를 구하는 작업 등에 쓰임  \n",
    "- TF-IDF 는 TF와 IDF를 곱한 값을 의미  \n",
    "- 문서를 d, 단어를 t, 문서의 총 개수를 n이라고 하면  \n",
    "1) tf(d, t): 특정 문서 d에서의 특정 단어 t의 등장 회수  \n",
    "2) df(t) : 특정 단어 t가 등장한 문서의 수  \n",
    "3) idf(d, t) : df(t)에 반비례하는 수  \n",
    "idf(d, t) = log(n / (1 + df(t)))\n",
    "- 총 문서의 수 n이 급격히 증가하게 되면 IDF의 값이 기하급수적으로 커지는 것을 방지하기 위해 log를 사용  \n",
    "- 특정 단어가 전체 문서에서 등장하지 않게 되는 경우 분모가 0이 되는 것을 방지하기 위해 1을 더함  \n",
    "- TF-IDF는 모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단  \n",
    "- 특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단  \n",
    "- TF-IDF 값이 낮으면 중요도가 낮다고 판단  \n",
    "- 예를 들어 영어에서 the 나 a와 같은 불용어의 경우 모든 문서에 자주 등장하기 때문에 이런 불용어들의 TF-IDF 값은 다른 단어에 비해서 낮아지게 됨  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "924f199c-a862-4038-8349-b050c50e6c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4526087d-8c53-4d4a-869f-f55f571f40b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dat = ['나는 배가 고프다', '내일 점심 뭐먹지', '내일 공부 해야겠다', '점심 먹고 공부 해야지']\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfa63de5-24ac-49e1-98b8-7c2a90d284df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나는': 2, '배가': 6, '고프다': 0, '내일': 3, '점심': 7, '뭐먹지': 5, '공부': 1, '해야겠다': 8, '먹고': 4, '해야지': 9}\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer.fit(text_data)\n",
    "print(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7d1130d-0e0d-4f9a-b6cc-72940a0cd06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.43779123 0.         0.         0.55528266 0.\n",
      "  0.         0.43779123 0.         0.55528266]]\n"
     ]
    }
   ],
   "source": [
    "sentence = [text_data[3]]\n",
    "print(tfidf_vectorizer.transform(sentence).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce00594a-7b92-494a-947d-8c7e41a44812",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-99a308e01177>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-99a308e01177>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    - TF-IDF 값을 사용할 경우, 단순 횟루를 이용하는 것보다 각 단어의 특성을 좀 더 잘 반영할 수 있음\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "- TF-IDF 값을 사용할 경우, 단순 횟루를 이용하는 것보다 각 단어의 특성을 좀 더 잘 반영할 수 있음\n",
    "- 모델에 적용할 때도 TfidVectorizer를 사용하는 것이 일반적으로 더 좋은 결과를 만들어냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f33db2-8530-4d0e-aacf-869ef82c4b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나누는 작업 -> 토큰 작업\n",
    "# 나누는 단위가 필요 : 글자 수, 등등\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

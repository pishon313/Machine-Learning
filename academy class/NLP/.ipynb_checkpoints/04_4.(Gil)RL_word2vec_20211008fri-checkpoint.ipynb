{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9452fdf4-81cd-40dd-be7d-8b221f99cd6e",
   "metadata": {},
   "source": [
    "### word2vec 을 이용한 모델\n",
    "- word2vec 은 단어로 표현된 리스트를 입력값으로 넣어야 함\n",
    "- 전처리된 텍스트를 불러온 후 각 단어들의 리스트로 나누어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b76d8c4-b07a-4359-a8d6-1bfbccf1fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# p 173 .시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05059572-3792-44ab-8db2-43a63b5d2c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stuff going moment mj started listening music ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>classic war worlds timothy hines entertaining ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>film starts manager nicholas bell giving welco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>must assumed praised film greatest filmed oper...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>superbly trashy wondrously unpretentious explo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  stuff going moment mj started listening music ...          1\n",
       "1  classic war worlds timothy hines entertaining ...          1\n",
       "2  film starts manager nicholas bell giving welco...          0\n",
       "3  must assumed praised film greatest filmed oper...          0\n",
       "4  superbly trashy wondrously unpretentious explo...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_IN_PATH = './data_in/'\n",
    "DATA_OUT_PATH = './data_out/'\n",
    "TRAIN_CLEAN_DATA = 'train_clean.csv'\n",
    "\n",
    "train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "558ccb3b-1afc-4f62-8c58-a810bc74e746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변환\n",
    "reviews = list(train_data['review'])\n",
    "sentiments = list(train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69733158-9166-4c17-aa50-8d14265f7e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7659406f-67b5-4d13-9138-24f5d5ae3c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stuff going moment mj started listening music watching odd documentary watched wiz watched moonwalker maybe want get certain insight guy thought really cool eighties maybe make mind whether guilty innocent moonwalker part biography part feature film remember going see cinema originally released subtle messages mj feeling towards press also obvious message drugs bad kay visually impressive course michael jackson unless remotely like mj anyway going hate find boring may call mj egotist consenting making movie mj fans would say made fans true really nice actual feature film bit finally starts minutes excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord wants mj dead bad beyond mj overheard plans nah joe pesci character ranted wanted people know supplying drugs etc dunno maybe hates mj music lots cool things like mj turning car robot whole speed demon sequence also director must patience saint came filming kiddy bad sequence usually directors hate working one kid let alone whole bunch performing complex dance scene bottom line movie people like mj one level another think people stay away try give wholesome message ironically mj bestest buddy movie girl michael jackson truly one talented people ever grace planet guilty well attention gave subject hmmm well know people different behind closed doors know fact either extremely nice stupid guy one sickest liars hope latter\n"
     ]
    }
   ],
   "source": [
    "print(reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79b0ddb6-1f50-4ce9-8d16-d44a7bacd374",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for review in reviews:\n",
    "    sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbc63942-828a-4bad-9203-d71fbb818ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stuff', 'going', 'moment', 'mj', 'started', 'listening', 'music', 'watching', 'odd', 'documentary', 'watched', 'wiz', 'watched', 'moonwalker', 'maybe', 'want', 'get', 'certain', 'insight', 'guy', 'thought', 'really', 'cool', 'eighties', 'maybe', 'make', 'mind', 'whether', 'guilty', 'innocent', 'moonwalker', 'part', 'biography', 'part', 'feature', 'film', 'remember', 'going', 'see', 'cinema', 'originally', 'released', 'subtle', 'messages', 'mj', 'feeling', 'towards', 'press', 'also', 'obvious', 'message', 'drugs', 'bad', 'kay', 'visually', 'impressive', 'course', 'michael', 'jackson', 'unless', 'remotely', 'like', 'mj', 'anyway', 'going', 'hate', 'find', 'boring', 'may', 'call', 'mj', 'egotist', 'consenting', 'making', 'movie', 'mj', 'fans', 'would', 'say', 'made', 'fans', 'true', 'really', 'nice', 'actual', 'feature', 'film', 'bit', 'finally', 'starts', 'minutes', 'excluding', 'smooth', 'criminal', 'sequence', 'joe', 'pesci', 'convincing', 'psychopathic', 'powerful', 'drug', 'lord', 'wants', 'mj', 'dead', 'bad', 'beyond', 'mj', 'overheard', 'plans', 'nah', 'joe', 'pesci', 'character', 'ranted', 'wanted', 'people', 'know', 'supplying', 'drugs', 'etc', 'dunno', 'maybe', 'hates', 'mj', 'music', 'lots', 'cool', 'things', 'like', 'mj', 'turning', 'car', 'robot', 'whole', 'speed', 'demon', 'sequence', 'also', 'director', 'must', 'patience', 'saint', 'came', 'filming', 'kiddy', 'bad', 'sequence', 'usually', 'directors', 'hate', 'working', 'one', 'kid', 'let', 'alone', 'whole', 'bunch', 'performing', 'complex', 'dance', 'scene', 'bottom', 'line', 'movie', 'people', 'like', 'mj', 'one', 'level', 'another', 'think', 'people', 'stay', 'away', 'try', 'give', 'wholesome', 'message', 'ironically', 'mj', 'bestest', 'buddy', 'movie', 'girl', 'michael', 'jackson', 'truly', 'one', 'talented', 'people', 'ever', 'grace', 'planet', 'guilty', 'well', 'attention', 'gave', 'subject', 'hmmm', 'well', 'know', 'people', 'different', 'behind', 'closed', 'doors', 'know', 'fact', 'either', 'extremely', 'nice', 'stupid', 'guy', 'one', 'sickest', 'liars', 'hope', 'latter']\n"
     ]
    }
   ],
   "source": [
    "# 결과\n",
    "print(sentences[0])\n",
    "# 블랭크로 다 짤라서 append 시킨 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6af8a309-c211-4beb-b756-96e041db2e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 하이퍼파라미터를 가지고 word2vec을 학습한다,\n",
    "# 학습에는 gensim 리이브러리를 사용한다.\n",
    "\n",
    "# conda install -c anaconda gensim\n",
    "\n",
    "# (책에는 pip install gensum 으로 나와있음)\n",
    "\n",
    "# gensim 라이브러리가 설치되었다면, gensim.models에 있는 word2vec 모듈을 불러와서 사용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ce82f-1713-4e1b-87f6-c2a5468e1ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e65073c0-e69d-4ca5-90dd-e2ffc91c9493",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300  #출력차원수\n",
    "min_word_count = 40\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9f8cae-ed3a-485b-8efc-f2e5b1b8613e",
   "metadata": {},
   "source": [
    "- num_features : 각 단어에 대해 임베딩된 벡터의 차원 지정(feature 수)\n",
    "- min_word_count : 모델에 의미 있는 단어를 가지고 학습하기 위해 적은 빈도 수의 단어들은 학습하지 않기 위해 설정  \n",
    "- num_workers : 모델 학습 시 학습을 위한 쓰레드 수 지정(기본값 3)  \n",
    "- context : word2vec 을 수행하기 위한 컨텍스트 윈도우 사이즈 지정  \n",
    "a. Maximum distance between the current and predicted word within a sentence.  \n",
    "b. 기준 단어의 앞뒤에 존재하는 단어들로 기준 단어를 예측하게 되는데(sg=0, CBOW-Continuous Bag of Words)  \n",
    "c. 이 때 기준 단어에서 앞뒤 얼마나 떨어져 있는 단어까지 고려하는가를 결정\n",
    "- downsampling : word2vec 학습을 수행할 때 빠른 학습을 위해 정답 단어 레이블에 대한 다운샘플링 비율을 지정  \n",
    "a. 보통 0.001이 좋은 성능을 낸다고 알려짐  \n",
    "b. 0.001 값을 threshold 값으로 보고, 이 값보다 빈도수가 높은 단어들은 무작위로(랜덤) 다운샘플링 됨  \n",
    "c. 빈도수가 높은 단어는 다운샘플링하여 가끔 학습(랜덤하게 무시)하고 빈도수가 낮은 단어는 출현 족족 학습하는 효과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "046cc8be-7c05-4a7c-93cd-86482fbd4921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec을 본격적으로 진행하기 전에 word2vec을 학습하는 과정에서 \n",
    "# 진행 상황을 확인해 보기 위해 다음과 같이 logging을 이용할 수 있다.\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',  #시간, levelname, 메세지를 문자열로 찍는것\n",
    "                    level = logging.INFO)                       # info level로 logging을 찍는 것.\n",
    "# 로깅logging에는 여러가지가 있음.\n",
    "# info, debug 등 어느 수준으로 로깅logging을 할거냐 라는걸 정해주는 것\n",
    "\n",
    "# 로깅을 할 때 format을 위와 같이 지정하고, 로그 수준은 INFO에 맞추면 \n",
    "# word2vec의 학습과 과정에서 로그 메시지를 양식에 맞게 INFO 수준으로 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09e524b8-c03a-4997-8d2a-e9cbba077bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-08 14:05:50,059 : INFO : collecting all words and their counts\n",
      "2021-10-08 14:05:50,060 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-08 14:05:50,309 : INFO : PROGRESS: at sentence #10000, processed 1205223 words, keeping 51374 word types\n",
      "2021-10-08 14:05:50,576 : INFO : PROGRESS: at sentence #20000, processed 2396605 words, keeping 67660 word types\n",
      "2021-10-08 14:05:50,696 : INFO : collected 74065 word types from a corpus of 2988089 raw words and 25000 sentences\n",
      "2021-10-08 14:05:50,697 : INFO : Loading a fresh vocabulary\n",
      "2021-10-08 14:05:50,749 : INFO : effective_min_count=40 retains 8160 unique words (11% of original 74065, drops 65905)\n",
      "2021-10-08 14:05:50,749 : INFO : effective_min_count=40 leaves 2627273 word corpus (87% of original 2988089, drops 360816)\n",
      "2021-10-08 14:05:50,779 : INFO : deleting the raw counts dictionary of 74065 items\n",
      "2021-10-08 14:05:50,781 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2021-10-08 14:05:50,783 : INFO : downsampling leaves estimated 2494384 word corpus (94.9% of prior 2627273)\n",
      "2021-10-08 14:05:50,807 : INFO : estimated required memory for 8160 words and 300 dimensions: 23664000 bytes\n",
      "2021-10-08 14:05:50,807 : INFO : resetting layer weights\n",
      "2021-10-08 14:05:52,669 : INFO : training model with 4 workers on 8160 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2021-10-08 14:05:53,677 : INFO : EPOCH 1 - PROGRESS: at 28.84% examples, 727495 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:05:54,679 : INFO : EPOCH 1 - PROGRESS: at 63.33% examples, 794246 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:05:55,540 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-08 14:05:55,546 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-08 14:05:55,546 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-08 14:05:55,552 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-08 14:05:55,553 : INFO : EPOCH - 1 : training on 2988089 raw words (2495054 effective words) took 2.9s, 867388 effective words/s\n",
      "2021-10-08 14:05:56,558 : INFO : EPOCH 2 - PROGRESS: at 41.46% examples, 1041915 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:05:57,559 : INFO : EPOCH 2 - PROGRESS: at 80.13% examples, 1000896 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:05:58,154 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-08 14:05:58,159 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-08 14:05:58,176 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-08 14:05:58,177 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-08 14:05:58,177 : INFO : EPOCH - 2 : training on 2988089 raw words (2494048 effective words) took 2.6s, 951955 effective words/s\n",
      "2021-10-08 14:05:59,191 : INFO : EPOCH 3 - PROGRESS: at 41.79% examples, 1040015 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:06:00,202 : INFO : EPOCH 3 - PROGRESS: at 82.05% examples, 1015872 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:06:00,677 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-08 14:06:00,685 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-08 14:06:00,699 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-08 14:06:00,701 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-08 14:06:00,701 : INFO : EPOCH - 3 : training on 2988089 raw words (2494489 effective words) took 2.5s, 989634 effective words/s\n",
      "2021-10-08 14:06:01,738 : INFO : EPOCH 4 - PROGRESS: at 36.03% examples, 882453 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:06:02,753 : INFO : EPOCH 4 - PROGRESS: at 71.94% examples, 882260 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:06:03,434 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-08 14:06:03,435 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-08 14:06:03,444 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-08 14:06:03,459 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-08 14:06:03,460 : INFO : EPOCH - 4 : training on 2988089 raw words (2494556 effective words) took 2.8s, 906167 effective words/s\n",
      "2021-10-08 14:06:04,468 : INFO : EPOCH 5 - PROGRESS: at 38.77% examples, 972103 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:06:05,480 : INFO : EPOCH 5 - PROGRESS: at 78.01% examples, 969013 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:06:05,973 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-08 14:06:05,979 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-08 14:06:05,979 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-08 14:06:05,984 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-08 14:06:05,985 : INFO : EPOCH - 5 : training on 2988089 raw words (2494013 effective words) took 2.5s, 989167 effective words/s\n",
      "2021-10-08 14:06:05,985 : INFO : training on a 14940445 raw words (12472160 effective words) took 13.3s, 936733 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# 이제 본격적으로 학습을 실행\n",
    "# word2vec 학습을 위해서는 word2vec 모듈에 있는 Word2Vec 객체를 생성해서 실행한다.\n",
    "# 이렇게 학습하고 생성된 객체는 model 변수에 할당한다.\n",
    "# 이때 학습을 위한 객체의 인자는 입력할 데이터와 하이퍼파라미터를 순서대로 입력해야 \n",
    "# 원하는 하이퍼파라미터를 사용해 학습할 수 있다.\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers,   # num_workers 등 위에서 정해줌\n",
    "                         size=num_features, min_count=min_word_count,\n",
    "                         window=context, sample=downsampling)\n",
    "\n",
    "# 다음은 학습이 진행되면서 나오는 로그 메시지다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8be3f7eb-e33a-44cb-bce8-39b4ce4863c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----> 7 from gensim.models import word2vec\n",
    "# ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject\n",
    "# 에러, 충돌 발생\n",
    "# 가상환경 새로 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ac21d15-1094-4963-9fbb-6a0c03e31a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 솔루션 (가상환경 안만들고 gensim 지우고 버전 낮춰서 다시 받으니까 됨)\n",
    "\n",
    "# conda uninstall gensim\n",
    "# conda intall -c anaconda gensim=3.8.3\n",
    "\n",
    "# 완료하고 위에 다시 실행하면 잘 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c60bdae-28be-4d0f-863f-0a19ec47f6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-08 14:06:06,041 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2021-10-08 14:06:06,043 : INFO : not storing attribute vectors_norm\n",
      "2021-10-08 14:06:06,044 : INFO : not storing attribute cum_table\n",
      "2021-10-08 14:06:06,195 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec13cdd9-6619-4cca-8ea3-3a137dd923df",
   "metadata": {},
   "source": [
    "- 위에서 만든 word2vec 모델을 활용하여 선형 회귀 모델을 학습시켜봄\n",
    "- 각 리뷰를 같은 형태의 입력값으로 만들어야 함\n",
    "- 리뷰마다 단어의 수가 모두 다르므로 입력값을 하나의 형태로 만듬\n",
    "- 가장 단순한 방법으로, 문장에 있는 모든 단어의 벡터값에 대해 평균을 내서 리뷰 하나당 하나의 벡터로 만드는 방법을 사용하겠음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d18f2ce0-a853-41a2-b146-5de939fd2bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(words, model, num_features):  # - words : 단어의 모음인 하나의 리뷰가 들어감 - model : word2vec 모델 - num_features : word2vec 으로 임베딩할 때 정했던 벡터의 차원 수\n",
    "# 결국 하나의 문장에 등장하는 사전에 등록된 단어들의 벡터값의 평균을 구함\n",
    "\n",
    "    # 출력 벡터를 초기화\n",
    "    feature_vector = np.zeros((num_features), dtype=np.float32)  # zero vector를 하나 만들어 놓고\n",
    "    \n",
    "    num_words = 0\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for w in words:   # 단어를 하나씩 꺼내서 단어사전 안에 존재하냐 확인\n",
    "        if w in index2word_set:   # 존재하면 하나 증가하고\n",
    "            num_words += 1\n",
    "            feature_vector = np.add(feature_vector, model[w])   # 벡터값을 꺼내서 더해줌\n",
    "            \n",
    "    feature_vector = np.divide(feature_vector, num_words)   # count한 단어의 갯수로 나눠주면 평균값이 됨\n",
    "    return feature_vector\n",
    "\n",
    "# 벡터값을 다 더해서 평균 내는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b7aa50-acb3-4f56-a70e-fba2de16e630",
   "metadata": {},
   "source": [
    "- words : 단어의 모음인 하나의 리뷰가 들어감\n",
    "- model : word2vec 모델\n",
    "- num_features : word2vec 으로 임베딩할 때 정했던 벡터의 차원 수\n",
    "- 결국 하나의 문장에 등장하는 사전에 등록된 단어들의 벡터값의 평균을 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74a9f316-e0c6-484a-a563-153927be3ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(reviews, model, num_features):\n",
    "    dataset = list()\n",
    "    \n",
    "    for s in reviews:   # 전체 리뷰에서 리뷰를 하나씩 끄집어 내서\n",
    "        dataset.append(get_features(s, model, num_features))   # append를 함, (get_feature~~)를 불러옴\n",
    "        \n",
    "    reviewFeatureVecs = np.stack(dataset)  # stack은 ()를 row로 쌓으면서 numpy 배열을 만듦\n",
    "    \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a47ed58b-cb31-4846-b55b-208cb87268d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드에 대한 설명"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d790e5-b65e-483e-a1d5-ddd7b62e9ef5",
   "metadata": {},
   "source": [
    "- review : 전체 리뷰 데이터\n",
    "- model : 학습시킨 모델\n",
    "- num_features : word2vec 임베딩 시 정했던 벡터의 차원 수\n",
    "- np.stack(dataset, axis=0) 은 row 로 데이터를 쌓으면서 numpy 배열을 만든다는 의미\n",
    "- 이렇게 하여 row가 전체 샘플 수 만큼, column 은 feature의 차원수가 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dafea29-40bd-4cc0-b2dc-33c417181108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_4088/3039896547.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  feature_vector = np.add(feature_vector, model[w])   # 벡터값을 꺼내서 더해줌\n"
     ]
    }
   ],
   "source": [
    "train_data_vecs = get_dataset(sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ad318a4-cd53-4b9d-9157-fa1a95f1b7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train_data_vecs\n",
    "y = np.array(sentiments)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "TEST_SPLIT = 0.2\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=TEST_SPLIT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43d2c216-b64b-40a1-a923-d20b1a02ff05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\python-env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lgs = LogisticRegression(class_weight='balanced')\n",
    "lgs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b63bdc9c-a854-4ce7-9ad0-5c08cd2e632a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.863600\n"
     ]
    }
   ],
   "source": [
    "predicted = lgs.predict(X_test)\n",
    "print('Accuracy: %f' % lgs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab87914e-e608-4ed3-9ab5-ebd997d24ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채점을 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b4ed6a3-a89f-4255-8d0c-b843c9424d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CLEAN_DATA = 'test_clean.csv'\n",
    "\n",
    "test_data = pd.read_csv(DATA_IN_PATH + TEST_CLEAN_DATA)\n",
    "test_review = list(test_data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f7333f1-cfbd-40bd-8241-995d20f37eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = list()\n",
    "for review in test_review:\n",
    "    test_sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830b2a5b-c39e-4205-aa1b-ee7b954c0202",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_4088/3039896547.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  feature_vector = np.add(feature_vector, model[w])   # 벡터값을 꺼내서 더해줌\n"
     ]
    }
   ],
   "source": [
    "test_data_vecs = get_dataset(test_sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5633d7e0-2ce1-41cd-9318-05ce2bb1b869",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

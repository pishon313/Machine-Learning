{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0bf5f20-86dd-44d6-ac62-df9f7b89e993",
   "metadata": {},
   "source": [
    "#word2vec 을 이용한 모델\n",
    "\n",
    "- word2vec은 단어로 표현된 리스트를 입력값으로 넣어야 함\n",
    "- 전처리된 텍스트를 불러온 후 각 단어들의 리스트로 나누어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8452964-8833-4c63-b752-2d01cd8d1921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e63ed355-ac89-4950-a60d-a2bb3ada67f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stuff going moment mj started listening music ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>classic war worlds timothy hines entertaining ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>film starts manager nicholas bell giving welco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>must assumed praised film greatest filmed oper...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>superbly trashy wondrously unpretentious explo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  stuff going moment mj started listening music ...          1\n",
       "1  classic war worlds timothy hines entertaining ...          1\n",
       "2  film starts manager nicholas bell giving welco...          0\n",
       "3  must assumed praised film greatest filmed oper...          0\n",
       "4  superbly trashy wondrously unpretentious explo...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_IN_PATH = './data_in/'\n",
    "DATA_OUT_PATH = './data_out/'\n",
    "TRAIN_CLEAN_DATA = 'train_clean.csv'\n",
    "\n",
    "train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa033574-0865-4ccc-acc3-4df85898faf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 문장문장을 단어들로 변환\n",
    "reviews = list(train_data['review'])\n",
    "sentiments = list(train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23a9f4a1-93b0-4845-83e6-6bb0a66ed1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95466418-8eef-45f4-89a9-0fc9db84c759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stuff going moment mj started listening music watching odd documentary watched wiz watched moonwalker maybe want get certain insight guy thought really cool eighties maybe make mind whether guilty innocent moonwalker part biography part feature film remember going see cinema originally released subtle messages mj feeling towards press also obvious message drugs bad kay visually impressive course michael jackson unless remotely like mj anyway going hate find boring may call mj egotist consenting making movie mj fans would say made fans true really nice actual feature film bit finally starts minutes excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord wants mj dead bad beyond mj overheard plans nah joe pesci character ranted wanted people know supplying drugs etc dunno maybe hates mj music lots cool things like mj turning car robot whole speed demon sequence also director must patience saint came filming kiddy bad sequence usually directors hate working one kid let alone whole bunch performing complex dance scene bottom line movie people like mj one level another think people stay away try give wholesome message ironically mj bestest buddy movie girl michael jackson truly one talented people ever grace planet guilty well attention gave subject hmmm well know people different behind closed doors know fact either extremely nice stupid guy one sickest liars hope latter\n"
     ]
    }
   ],
   "source": [
    "print(reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "538cc9b6-9e11-4a4a-ba06-91e032fe820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for review in reviews:\n",
    "    sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07f6ba0a-abc0-42e8-9f83-fb77bb81ff0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stuff', 'going', 'moment', 'mj', 'started', 'listening', 'music', 'watching', 'odd', 'documentary', 'watched', 'wiz', 'watched', 'moonwalker', 'maybe', 'want', 'get', 'certain', 'insight', 'guy', 'thought', 'really', 'cool', 'eighties', 'maybe', 'make', 'mind', 'whether', 'guilty', 'innocent', 'moonwalker', 'part', 'biography', 'part', 'feature', 'film', 'remember', 'going', 'see', 'cinema', 'originally', 'released', 'subtle', 'messages', 'mj', 'feeling', 'towards', 'press', 'also', 'obvious', 'message', 'drugs', 'bad', 'kay', 'visually', 'impressive', 'course', 'michael', 'jackson', 'unless', 'remotely', 'like', 'mj', 'anyway', 'going', 'hate', 'find', 'boring', 'may', 'call', 'mj', 'egotist', 'consenting', 'making', 'movie', 'mj', 'fans', 'would', 'say', 'made', 'fans', 'true', 'really', 'nice', 'actual', 'feature', 'film', 'bit', 'finally', 'starts', 'minutes', 'excluding', 'smooth', 'criminal', 'sequence', 'joe', 'pesci', 'convincing', 'psychopathic', 'powerful', 'drug', 'lord', 'wants', 'mj', 'dead', 'bad', 'beyond', 'mj', 'overheard', 'plans', 'nah', 'joe', 'pesci', 'character', 'ranted', 'wanted', 'people', 'know', 'supplying', 'drugs', 'etc', 'dunno', 'maybe', 'hates', 'mj', 'music', 'lots', 'cool', 'things', 'like', 'mj', 'turning', 'car', 'robot', 'whole', 'speed', 'demon', 'sequence', 'also', 'director', 'must', 'patience', 'saint', 'came', 'filming', 'kiddy', 'bad', 'sequence', 'usually', 'directors', 'hate', 'working', 'one', 'kid', 'let', 'alone', 'whole', 'bunch', 'performing', 'complex', 'dance', 'scene', 'bottom', 'line', 'movie', 'people', 'like', 'mj', 'one', 'level', 'another', 'think', 'people', 'stay', 'away', 'try', 'give', 'wholesome', 'message', 'ironically', 'mj', 'bestest', 'buddy', 'movie', 'girl', 'michael', 'jackson', 'truly', 'one', 'talented', 'people', 'ever', 'grace', 'planet', 'guilty', 'well', 'attention', 'gave', 'subject', 'hmmm', 'well', 'know', 'people', 'different', 'behind', 'closed', 'doors', 'know', 'fact', 'either', 'extremely', 'nice', 'stupid', 'guy', 'one', 'sickest', 'liars', 'hope', 'latter']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651cf8ee-be91-4c73-8e5b-8aba4e4ec1f4",
   "metadata": {},
   "source": [
    "conda install -c anaconda gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2253999-cb0f-4468-bb99-17d55d05aa4b",
   "metadata": {},
   "source": [
    "- num_features : 각 단어에 대해 임베딩된 벡터의 차원 지정(feature 수)\n",
    "- min_word_count : 모델에 의미 있는 단어를 가지고 학습하기 위해 적은 빈도 수의 단어들은 학습하지 않기 위해 설정  \n",
    "- num_workers : 모델 학습 시 학습을 위한 쓰레드 수 지정(기본값 3)  \n",
    "- context : word2vec 을 수행하기 위한 컨텍스트 윈도우 사이즈 지정  \n",
    "a. Maximum distance between the current and predicted word within a sentence.  \n",
    "b. 기준 단어의 앞뒤에 존재하는 단어들로 기준 단어를 예측하게 되는데(sg=0, CBOW-Continuous Bag of Words)  \n",
    "c. 이 때 기준 단어에서 앞뒤 얼마나 떨어져 있는 단어까지 고려하는가를 결정\n",
    "- downsampling : word2vec 학습을 수행할 때 빠른 학습을 위해 정답 단어 레이블에 대한 다운샘플링 비율을 지정  \n",
    "a. 보통 0.001이 좋은 성능을 낸다고 알려짐  \n",
    "b. 0.001 값을 threshold 값으로 보고, 이 값보다 빈도수가 높은 단어들은 무작위로(랜덤) 다운샘플링 됨  \n",
    "c. 빈도수가 높은 단어는 다운샘플링하여 가끔 학습(랜덤하게 무시)하고 빈도수가 낮은 단어는 출현 족족 학습하는 효과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78d390b8-fec7-410a-8133-a26b038991aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300\n",
    "min_word_count = 40\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9da1405-33cf-4420-ba13-eb3fea6209a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어느 정도 수준까지 logging 을 할 것인지 (실제 현장에선 많이 쓴다 )\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : $(levelname)s : %(message)s',\n",
    "                   level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35c06073-25ab-4c8f-a37d-10b15f3d0868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-08 14:16:56,403 : $(levelname)s : collecting all words and their counts\n",
      "2021-10-08 14:16:56,404 : $(levelname)s : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-10-08 14:16:56,608 : $(levelname)s : PROGRESS: at sentence #10000, processed 1205223 words, keeping 51374 word types\n",
      "2021-10-08 14:16:56,804 : $(levelname)s : PROGRESS: at sentence #20000, processed 2396605 words, keeping 67660 word types\n",
      "2021-10-08 14:16:56,900 : $(levelname)s : collected 74065 word types from a corpus of 2988089 raw words and 25000 sentences\n",
      "2021-10-08 14:16:56,901 : $(levelname)s : Loading a fresh vocabulary\n",
      "2021-10-08 14:16:56,936 : $(levelname)s : effective_min_count=40 retains 8160 unique words (11% of original 74065, drops 65905)\n",
      "2021-10-08 14:16:56,936 : $(levelname)s : effective_min_count=40 leaves 2627273 word corpus (87% of original 2988089, drops 360816)\n",
      "2021-10-08 14:16:56,954 : $(levelname)s : deleting the raw counts dictionary of 74065 items\n",
      "2021-10-08 14:16:56,956 : $(levelname)s : sample=0.001 downsamples 30 most-common words\n",
      "2021-10-08 14:16:56,957 : $(levelname)s : downsampling leaves estimated 2494384 word corpus (94.9% of prior 2627273)\n",
      "2021-10-08 14:16:56,973 : $(levelname)s : estimated required memory for 8160 words and 300 dimensions: 23664000 bytes\n",
      "2021-10-08 14:16:56,974 : $(levelname)s : resetting layer weights\n",
      "2021-10-08 14:16:58,272 : $(levelname)s : training model with 4 workers on 8160 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2021-10-08 14:16:59,280 : $(levelname)s : EPOCH 1 - PROGRESS: at 58.90% examples, 1476646 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:16:59,945 : $(levelname)s : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-08 14:16:59,950 : $(levelname)s : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-08 14:16:59,950 : $(levelname)s : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-08 14:16:59,961 : $(levelname)s : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-08 14:16:59,962 : $(levelname)s : EPOCH - 1 : training on 2988089 raw words (2494021 effective words) took 1.7s, 1480045 effective words/s\n",
      "2021-10-08 14:17:00,969 : $(levelname)s : EPOCH 2 - PROGRESS: at 58.90% examples, 1477713 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:17:01,623 : $(levelname)s : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-08 14:17:01,626 : $(levelname)s : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-08 14:17:01,628 : $(levelname)s : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-08 14:17:01,630 : $(levelname)s : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-08 14:17:01,630 : $(levelname)s : EPOCH - 2 : training on 2988089 raw words (2494611 effective words) took 1.7s, 1499495 effective words/s\n",
      "2021-10-08 14:17:02,635 : $(levelname)s : EPOCH 3 - PROGRESS: at 57.92% examples, 1455501 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:17:03,370 : $(levelname)s : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-08 14:17:03,373 : $(levelname)s : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-08 14:17:03,375 : $(levelname)s : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-08 14:17:03,387 : $(levelname)s : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-08 14:17:03,387 : $(levelname)s : EPOCH - 3 : training on 2988089 raw words (2494393 effective words) took 1.8s, 1422839 effective words/s\n",
      "2021-10-08 14:17:04,401 : $(levelname)s : EPOCH 4 - PROGRESS: at 55.57% examples, 1386452 words/s, in_qsize 8, out_qsize 0\n",
      "2021-10-08 14:17:05,115 : $(levelname)s : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-08 14:17:05,118 : $(levelname)s : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-08 14:17:05,120 : $(levelname)s : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-08 14:17:05,128 : $(levelname)s : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-08 14:17:05,129 : $(levelname)s : EPOCH - 4 : training on 2988089 raw words (2493683 effective words) took 1.7s, 1436033 effective words/s\n",
      "2021-10-08 14:17:06,134 : $(levelname)s : EPOCH 5 - PROGRESS: at 59.25% examples, 1487945 words/s, in_qsize 7, out_qsize 0\n",
      "2021-10-08 14:17:06,784 : $(levelname)s : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-08 14:17:06,788 : $(levelname)s : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-08 14:17:06,790 : $(levelname)s : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-08 14:17:06,804 : $(levelname)s : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-08 14:17:06,804 : $(levelname)s : EPOCH - 5 : training on 2988089 raw words (2494307 effective words) took 1.7s, 1492614 effective words/s\n",
      "2021-10-08 14:17:06,805 : $(levelname)s : training on a 14940445 raw words (12471015 effective words) took 8.5s, 1461784 effective words/s\n"
     ]
    }
   ],
   "source": [
    "import smart_open\n",
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, workers = num_workers, \n",
    "                         size = num_features, min_count = min_word_count,\n",
    "                         window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e489ab05-3dc9-4a5a-8e43-8c726aa8aa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-08 14:17:06,809 : $(levelname)s : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2021-10-08 14:17:06,810 : $(levelname)s : not storing attribute vectors_norm\n",
      "2021-10-08 14:17:06,811 : $(levelname)s : not storing attribute cum_table\n",
      "2021-10-08 14:17:06,967 : $(levelname)s : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# 여기까지 한 내용 저장하기\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b7b3daf-ea67-457e-b3a5-2f7bffe75fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형 회귀 모델을 훈련시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360b4cf5-afdd-4a36-9a3a-5d2243bd2cf6",
   "metadata": {},
   "source": [
    "- 위에서 많은 word2vec 모델을 활용하여 선형 회귀 모델을 학습시켜봄\n",
    "- 각 리뷰를 같은 형태의 입력값으로 만들어야 함\n",
    "- 기뷰마다 단어의 수가 모두 다르므로 입력값을 하나의 형태로 만듬\n",
    "- 가장 단순한 방법으로, 문장에 있는 모든 단어의 벡터값에 대한 평균을 내서 리뷰 하나당 하나의 벡터로 만드는 방법을 사용하겠음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15841d4b-e3cc-4f27-8133-026219fcbc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(words, model, num_features):\n",
    "    feature_vector = np.zeros((num_features), dtype = np.float32)\n",
    "    \n",
    "    num_words = 0\n",
    "    index2word_set = set(model.wv.index2word) # index2word : 단어 사전\n",
    "    \n",
    "    for w in words:  # 단어를 하나씩 꺼내서 단어 사전에 존재한다면, 추가시키고, 벡터값을 꺼내서 곱해준다 \n",
    "        if w in index2word_set:\n",
    "            num_words += 1\n",
    "            feature_vector = np.add(feature_vector, model[w])\n",
    "            \n",
    "        feature_vector = np.divide(feature_vector, num_words) # 단어별로 벡터값이 있는데, 이걸 다 더해서 평균을 낸 것\n",
    "        return feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a69814f-d816-4bfe-94e6-96363d205da9",
   "metadata": {},
   "source": [
    "- words : 단어의 모음인 하나의 리뷰가 들어감\n",
    "- model : word2vec 모델\n",
    "- num_features : word2vec 으로 임베딩할 때 정했던 벡터의 차원 수  \n",
    "- 결극 하나의 문장에 등장하는 사전에 등록된 단어들의 벡터값의 평균을 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "917f4ae7-9811-4500-9118-974b29006df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(review, model, num_features):\n",
    "    dataset = list()\n",
    "    \n",
    "    for s in reviews:\n",
    "        dataset.append(get_features(words, model, num_features))\n",
    "        # 이렇게 해서 나온 평균 벡터값을 계속 추가해 준다. \n",
    "        \n",
    "        reveiwFeatureVecs = np.stack(dataset)\n",
    "        \n",
    "        return reveiwFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e84886e-8b02-4865-b732-428dd855b371",
   "metadata": {},
   "source": [
    "- review : 전체 리뷰 데이터  \n",
    "- model : 학습시킨 모델  \n",
    "- num_features : word2vec 임베딩 시 정했던 벡터의 차원 수  \n",
    "- np.stack(dataset, axis=0) 은 row 로 데이터를 쌓으면서 numpy 배열을 만든다는 의미  \n",
    "- 이렇게 하여 row가 전체 샘플 수 만큼, column 은 feature의 차원 수가 됨  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23e5862c-e969-4396-9cf9-7f5739b79d1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z8/rs4prgjd5fx5l8q5jfkbyt2r0000gn/T/ipykernel_4044/1797831137.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/z8/rs4prgjd5fx5l8q5jfkbyt2r0000gn/T/ipykernel_4044/3817547252.py\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(review, model, num_features)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m# 이렇게 해서 나온 평균 벡터값을 계속 추가해 준다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "train_data_vecs = get_dataset(sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268930cd-ee43-4768-9978-5f3211519aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lgs = LogisticRegression(class_weight='balanced')\n",
    "lgs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bf52a9-6f39-4fbe-b630-0ae895d4645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = lgs.predict(X_test)\n",
    "print('Accuracy : %f' % lgs.socre(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1549b8-3c05-4c57-b51a-e9d71f0bf258",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CLEAN_DATA = 'test_clean.csv'\n",
    "\n",
    "test_data = pd.read_csv(DATA_IN_PATH + TEST_CLEAN_DATA)\n",
    "test_review = list(test_data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240aaa4e-021b-4266-96f1-c7f43a0bea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = list()\n",
    "for review in test_review:\n",
    "    test_sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157411ca-b65b-43da-b405-f102615d1a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_vecs = get_dataset(test_sentences, model, num_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0191a4-7c23-46d0-8937-84994e1fdaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted = lgs.predict(test_data_vecs)\n",
    "\n",
    "ids = list(test_data['id'])\n",
    "answer_dataset = pd.DataFrame({'id': ids, 'sentiment': test_predicted})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276884a6-7b6f-44d7-a056-fc017414892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afe59a4-11d0-4bfb-9c50-37181cc681f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
